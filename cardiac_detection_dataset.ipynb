{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47ced4b7-411f-49eb-93c7-01b2be562288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import albumentations as A # \n",
    "from albumentations.pytorch import ToTensorV2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5a1e37b-8450-4142-be49-0e86b6d7b583",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = \"./rsna_heart_detection.csv\"\n",
    "labels =  pd.read_csv(labels_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f55701e4-8ab0-4b74-942d-069f99197e60",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "can only convert an array of size 1 to a Python scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/lib/python3.10/site-packages/pandas/core/base.py:422\u001b[0m, in \u001b[0;36mIndexOpsMixin.item\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n\u001b[0;32m--> 422\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan only convert an array of size 1 to a Python scalar\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: can only convert an array of size 1 to a Python scalar"
     ]
    }
   ],
   "source": [
    "labels[\"x0\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84fcb743-3cf6-4eb8-8457-ec93ddc3a886",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardiacDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __ini__(self, path_to_labels_csv, patients, root_path, augs):\n",
    "\n",
    "        self.labels = pd.read_csv(path_to_labels_csv)\n",
    "        self.patients = np.load(patients) # array with patient ID names\n",
    "        self.root_path = Path(root_path)\n",
    "        self.augment = augs # augmentation pipeline\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset \n",
    "        \"\"\"\n",
    "        return len(self.patients)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns an image paired with the corresponding bounding box around the heart\n",
    "        \"\"\"\n",
    "        # get patient data\n",
    "        patient = self.patients[idx] # get data according to index\n",
    "        data = self.labels[[\"name\"]==patient]\n",
    "\n",
    "        #extract bounding box coordinates \n",
    "        x_min = data[\"x0\"].item() # left edge\n",
    "        y_min = data[\"y0\"].item() # top edge\n",
    "        x_max = x_min + data[\"w\"].item() # right edge\n",
    "        y_max = y_min + data[\"h\"].item() # bottom edge\n",
    "\n",
    "        #Load image \n",
    "        file_path = self.root_path/patient\n",
    "        img = np.load(f\"{file_path}.npy\").astype(np.float32) # Converts to float32 for NN processing\n",
    "\n",
    "        # apply augmentations\n",
    "        if self.augment:\n",
    "            height , width = img.shape[:2] # image dimensions\n",
    "            bboxes = [[x_min, y_min, x_max, y_max]]\n",
    "            # Apply augmentations\n",
    "            augmented = self.augment(\n",
    "                image=img,\n",
    "                bboxes=bboxes,\n",
    "                class_labels=['heart']  # Albumentations needs class labels for bboxes\n",
    "\n",
    "            )\n",
    "\n",
    "            img = augmented['image']\n",
    "\n",
    "            # Get the augmented bounding box\n",
    "            if len(augmented['bboxes']) > 0:\n",
    "                aug_bbox = augmented['bboxes'][0]  # Get first (and only) bbox\n",
    "                bbox = [aug_bbox[0], aug_bbox[1], aug_bbox[2], aug_bbox[3]]\n",
    "            else:\n",
    "                # If bbox was cropped out, use original\n",
    "                bbox = [x_min, y_min, x_max, y_max]\n",
    "        else:\n",
    "            bbox = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "        # Normalize the image according to the mean and std deviation (in preprocessing file)\n",
    "        img = (img - 0.494) / 0.252\n",
    "\n",
    "        # Convert to tensor and add channel dimension for grayscale\n",
    "        img = torch.tensor(img).unsqueeze(0)\n",
    "        bbox = torch.tensor(bbox, dtype=torch.float32)\n",
    "\n",
    "        return img, bbox\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1884e6c-c487-4914-ae11-49a62ddabf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an augmentation object\n",
    "seq = A.Compose([\n",
    "    # Gamma/contrast adjustments\n",
    "    A.OneOf([\n",
    "        A.RandomGamma(gamma_limit=(80, 120), p=1.0),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=1.0),\n",
    "    ], p=0.8),\n",
    "    \n",
    "    # Geometric transformations\n",
    "    A.Affine(\n",
    "        scale={'x': (0.8, 1.2), 'y': (0.8, 1.2)}, \n",
    "        rotate=(-10, 10),\n",
    "        translate_px=(-10, 10),\n",
    "        shear=(-5, 5),  # add slight shearing\n",
    "        p=0.9\n",
    "    ),\n",
    "    \n",
    "], bbox_params=A.BboxParams(\n",
    "    format='pascal_voc',\n",
    "    label_fields=['class_labels']\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3830be-82de-4003-829b-3d946b5a9067",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
